{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f435793",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:09:38.944204Z",
     "start_time": "2022-06-24T11:09:38.930202Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils_functions import *\n",
    "from plan import *\n",
    "import random\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import datetime, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "349ba869",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:09:40.720180Z",
     "start_time": "2022-06-24T11:09:40.712180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "#controllo di stare effettivamente usando la GPU\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7177344b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:09:47.346604Z",
     "start_time": "2022-06-24T11:09:42.041606Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded\n",
      "File loaded\n"
     ]
    }
   ],
   "source": [
    "dict_stati= load_file(\"./dizionario_stati\")\n",
    "plansLoaded=load_file(\"./plans\")\n",
    "#print(type(plansPickles))\n",
    "#plansList=load_from_pickles(\"C:/Users/Francesco/Desktop/dataset/dataset/dizionario_stati\",plansPickles)\n",
    "#print(plansPickles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "994cd7db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:09:47.362104Z",
     "start_time": "2022-06-24T11:09:47.348104Z"
    }
   },
   "outputs": [],
   "source": [
    "#ogni stato è un tensore con elementi dtype int8, questi stati vengono raggruppati in un altro tensore che rappresenta la\n",
    "#variabile \"states\" del piano \n",
    "\n",
    "def build_vector(dict, states_list):\n",
    "    l=len(dict)\n",
    "    vector_states=[]\n",
    "    for state in states_list:\n",
    "        vector=np.array([0]*l,dtype=np.int8)\n",
    "        #vector=tf.zeros(l,dtype=np.int8)\n",
    "        for s in state:\n",
    "            for key in dict.keys():\n",
    "                if key==s:\n",
    "                    vector[dict[key]-1]=1\n",
    "                    break\n",
    "        #t=tf.convert_to_tensor(vector,dtype=tf.int8)            \n",
    "        vector_states.append(vector) \n",
    "    r = np.array(vector_states)                          \n",
    "    return r\n",
    "\n",
    "\n",
    "#NON UTILIZZATO ADESSO\n",
    "#shape di ogni singolo elemento (r) è (n x 340) con n che varia su ogni piano, raggruppati sulla base dei piani\n",
    "\n",
    "def build_all_vectors(dict,plans_list):\n",
    "    total=[]\n",
    "    for plan in plans_list:\n",
    "        r=build_vector(dict,plan.states)\n",
    "        total.append(r)\n",
    "    #r = tf.convert_to_tensor(total,dtype=None)                         \n",
    "    return total\n",
    " \n",
    "    \n",
    "#gli stati vengono ordinati (in una lista) singolarmente con shape (1x340), non vengono raggruppati sulla base dei piani\n",
    "#da utilizzare per autoencoder standard\n",
    "\n",
    "def build_all_vectors1x340(dict, plans_list):\n",
    "    l=len(dict)\n",
    "    total=[]\n",
    "    for plan in plans_list:\n",
    "        for state in plan.states:\n",
    "            vector=np.array([0]*l,dtype=np.int8)\n",
    "            for s in state:\n",
    "                for key in dict.keys():\n",
    "                    if key==s:\n",
    "                        vector[dict[key]-1]=1\n",
    "                        break\n",
    "            #t=tf.convert_to_tensor(vector,dtype=tf.int8)            \n",
    "            total.append(vector)\n",
    "    total=np.array(total)        \n",
    "    return total        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8f80382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:09:47.377604Z",
     "start_time": "2022-06-24T11:09:47.364107Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all_states_from_plans_list=build_all_vectors(dict_stati,plansLoaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe83a51",
   "metadata": {},
   "source": [
    "## Preparazione dati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c8c33",
   "metadata": {},
   "source": [
    "### Caricamento dataset statici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a1464b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:12:20.049604Z",
     "start_time": "2022-06-24T11:09:47.379106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded\n",
      "File loaded\n",
      "File loaded\n"
     ]
    }
   ],
   "source": [
    "test=load_file(\"./Dataset/set_test\")\n",
    "train=load_file(\"./Dataset/set_training\")\n",
    "validation=load_file(\"./Dataset/set_validation\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2eda145f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T09:50:06.261686Z",
     "start_time": "2022-06-24T09:49:21.422688Z"
    }
   },
   "outputs": [],
   "source": [
    "test=np.array(test,dtype=np.int8)\n",
    "train=np.array(train,dtype=np.int8)\n",
    "validation=np.array(validation,dtype=np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85be87a",
   "metadata": {},
   "source": [
    "## Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00dc9c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T11:40:45.849108Z",
     "start_time": "2022-06-24T11:40:45.687645Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'hidden_size': 160}\n",
      "Arriva fin qui e perchè non prosegue? Come faccio a fargli piacere hparams? Ma soprattutto, è davvero colpa di hparams?\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 340), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") at layer \"dense\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 131>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mprint\u001b[39m({h\u001b[38;5;241m.\u001b[39mname: hparams[h] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hparams})\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArriva fin qui e perchè non prosegue? Come faccio a fargli piacere hparams? Ma soprattutto, è davvero colpa di hparams?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 139\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogs/hparam_tuning/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m session_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[1;32mIn [47]\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_dir, hparams)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mcreate_file_writer(run_dir)\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m     99\u001b[0m     hp\u001b[38;5;241m.\u001b[39mhparams(hparams)  \u001b[38;5;66;03m# record the values used in this trial\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     tf\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mscalar(METRIC_ACCURACY, accuracy, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Input \u001b[1;32mIn [47]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(hparams)\u001b[0m\n\u001b[0;32m     87\u001b[0m code \u001b[38;5;241m=\u001b[39m Dense(code_size, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mswish\u001b[39m\u001b[38;5;124m'\u001b[39m,kernel_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhe_uniform\u001b[39m\u001b[38;5;124m\"\u001b[39m,)(hidden_1)\n\u001b[0;32m     88\u001b[0m hidden_2 \u001b[38;5;241m=\u001b[39mDense(hparams[HP_HIDDEN_SIZE], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mswish\u001b[39m\u001b[38;5;124m'\u001b[39m,kernel_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhe_uniform\u001b[39m\u001b[38;5;124m\"\u001b[39m,)(code)\n\u001b[1;32m---> 89\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Che siano precision e recall a dare problemi, visto che prima ho definito solo l'accuracy come tunabile?\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy',metrics=[\"accuracy\",\"Precision\",\"Recall\"])\u001b[39;00m\n\u001b[0;32m     92\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\EnvAutoEncoder\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:530\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 530\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\EnvAutoEncoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:115\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[1;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m generic_utils\u001b[38;5;241m.\u001b[39mvalidate_kwargs(kwargs, {})\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28msuper\u001b[39m(Functional, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name\u001b[38;5;241m=\u001b[39mname, trainable\u001b[38;5;241m=\u001b[39mtrainable)\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_graph_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\EnvAutoEncoder\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:530\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 530\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\EnvAutoEncoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:198\u001b[0m, in \u001b[0;36mFunctional._init_graph_network\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_coordinates\u001b[38;5;241m.\u001b[39mappend((layer, node_index, tensor_index))\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# Keep track of the network's nodes and layers.\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m nodes, nodes_by_depth, layers, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_map_graph_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_nodes \u001b[38;5;241m=\u001b[39m nodes\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes_by_depth \u001b[38;5;241m=\u001b[39m nodes_by_depth\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\EnvAutoEncoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:985\u001b[0m, in \u001b[0;36m_map_graph_network\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten(node\u001b[38;5;241m.\u001b[39mkeras_inputs):\n\u001b[0;32m    984\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(x) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m computable_tensors:\n\u001b[1;32m--> 985\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGraph disconnected: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    986\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcannot obtain value for tensor \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    987\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m layer\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    988\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe following previous layers \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    989\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwere accessed without issue: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    990\u001b[0m                      \u001b[38;5;28mstr\u001b[39m(layers_with_complete_input))\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten(node\u001b[38;5;241m.\u001b[39moutputs):\n\u001b[0;32m    992\u001b[0m   computable_tensors\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(x))\n",
      "\u001b[1;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 340), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") at layer \"dense\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "logdir = os.path.join(\"./TestLogs/\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=10),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=logdir,histogram_freq=1, write_graph=True, write_images=True)\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Cose da fare per migliorare la rete solo dopo aver fatto i primi tentativi con la rete proposta:\n",
    " x Provare ad aggiungere Regolarizzazione es. L1,L2 e dropout(solo nella fase di encoding)\n",
    " x Provare swish al posto di relu\n",
    " x Provare tf.keras.layers.BatchNormalization()(var_layer) \n",
    " ? Fare Hyperparameter Tuning  (ultima) www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams\n",
    " x Salvare i risultati con ogni modifica fatta per scriverli nel report\n",
    "\"\"\"\n",
    "\n",
    "input_size = 340 #costante\n",
    "\n",
    "# hidden_size = 170\n",
    "# #hidden_size2 = 85\n",
    "# code size non è commentato perchè ho provato a definirlo statico per la prova ultra basilare\n",
    "code_size = 85\n",
    "\n",
    "\n",
    "# Intervalli giocattolo per testare alla svelta se funziona.\n",
    "# Prova base: solo hidden size tunabile e accuracy\n",
    "\n",
    "HP_HIDDEN_SIZE = hp.HParam('hidden_size', hp.Discrete([160, 180]))\n",
    "# HP_CODE_SIZE = hp.HParam('code_size', hp.Discrete([80, 90]))\n",
    "# # HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu', 'swish']))\n",
    "# HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.001, 0.003))\n",
    "# HP_EPOCHS = hp.HParam('epochs', hp.Discrete([1400, 1600]))\n",
    "# HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([99000, 100000]))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "# METRIC_PRECISION = 'Precision'\n",
    "# METRIC_RECALL = 'Recall'\n",
    "\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        # Prova base: solo hidden size tunabile\n",
    "        # hparams=[HP_HIDDEN_SIZE, HP_CODE_SIZE, HP_ACTIVATION, HP_LEARNING_RATE, HP_EPOCHS, HP_BATCH_SIZE],\n",
    "        hparams=[HP_HIDDEN_SIZE],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "Codice statico temporaneamente commentato\n",
    "\n",
    "# input_layer = Input(shape=(input_size,))\n",
    "# hidden_1 = Dense(hidden_size, activation='swish', kernel_initializer=\"he_uniform\", )(input_layer)\n",
    "# #hidden_3 = Dense(hidden_size, activation='relu', kernel_initializer=\"he_uniform\")(hidden_1)\n",
    "\n",
    "# code = Dense(code_size, activation='swish',kernel_initializer=\"he_uniform\",)(hidden_1)\n",
    "# hidden_2 =Dense(hidden_size, activation='swish',kernel_initializer=\"he_uniform\",)(code)\n",
    "# #hidden_4 =Dense(hidden_size, activation='relu',kernel_initializer=\"he_uniform\")(hidden_2)\n",
    "\n",
    "# output_layer = Dense(input_size, activation='sigmoid')(hidden_2)\n",
    "\n",
    "# autoencoder = Model(input_layer, output_layer)\n",
    "# #variare learning_rate beta_1,beta_2, da fare per ultimo\n",
    "# #batch_size ???\n",
    "# autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy',metrics=[\"accuracy\",\"Precision\",\"Recall\"])\n",
    "# autoencoder.fit(x=train,y=train, epochs=500,batch_size=300000, validation_data=(validation,validation), callbacks=my_callbacks,\n",
    "#                       )\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Versione con tutti i parametri tunabili, temporaneamente commentata\n",
    "\n",
    "# def train_model(hparams):\n",
    "#     input_layer = Input(shape=(input_size,))\n",
    "#     hidden_1 = Dense(hparams[HP_HIDDEN_SIZE], activation='swish', kernel_initializer=\"he_uniform\", )(input_layer)\n",
    "#     code = Dense(hparams[HP_CODE_SIZE], activation='swish',kernel_initializer=\"he_uniform\",)(hidden_1)\n",
    "#     hidden_2 =Dense(hparams[HP_HIDDEN_SIZE], 'swish',kernel_initializer=\"he_uniform\",)(code)\n",
    "#     autoencoder = Model(input_layer, output_layer)\n",
    "#     #per ora faccio solo accuracy per testare il funzionamento\n",
    "#     autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hparams[HP_LEARNING_RATE]), loss='binary_crossentropy',metrics=['accuracy'])\n",
    "#     autoencoder.fit(x=train,y=train, epochs=hparams[HP_EPOCHS],batch_size=hparams[HP_BATCH_SIZE], validation_data=(validation,validation), callbacks=my_callbacks,\n",
    "#                    )\n",
    "#     return accuracy\n",
    "\n",
    "\n",
    "def train_model(hparams):\n",
    "    input_layer = Input(shape=(input_size,))\n",
    "    hidden_1 = Dense(hparams[HP_HIDDEN_SIZE], activation='swish', kernel_initializer=\"he_uniform\", )(input_layer)\n",
    "    code = Dense(code_size, activation='swish',kernel_initializer=\"he_uniform\",)(hidden_1)\n",
    "    hidden_2 =Dense(hparams[HP_HIDDEN_SIZE], 'swish',kernel_initializer=\"he_uniform\",)(code)\n",
    "    autoencoder = Model(input_layer, output_layer)\n",
    "    # Che siano precision e recall a dare problemi, visto che prima ho definito solo l'accuracy come tunabile? Nope, non va lo stesso\n",
    "    # autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy',metrics=[\"accuracy\",\"Precision\",\"Recall\"])\n",
    "    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy',metrics=[\"accuracy\"])\n",
    "    autoencoder.fit(x=train,y=train, epochs=500,batch_size=300000, validation_data=(validation,validation), callbacks=my_callbacks,\n",
    "                      )\n",
    "\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        accuracy = train_model(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "\n",
    "        \n",
    "session_num = 0\n",
    "\n",
    "# Versione con tutti i parametri tunabili, temporaneamente commentata\n",
    "\n",
    "# for num_hid in HP_HIDDEN_SIZE.domain.values:\n",
    "#     for num_code in HP_CODE_SIZE.domain.values:\n",
    "#         for acts in HP_ACTIVATION.domain.values:\n",
    "#             for num_lr in (HP_LEARNING_RATE.domain.min_value, HP_LEARNING_RATE.domain.max_value):\n",
    "#                 for num_ep in HP_EPOCHS.domain.values:\n",
    "#                     for num_bs in HP_BATCH_SIZE.domain.values:\n",
    "#                         hparams = {\n",
    "#                             HP_HIDDEN_SIZE: num_hid,\n",
    "#                             HP_CODE_SIZE: num_code,\n",
    "#                             HP_ACTIVATION: acts,\n",
    "#                             HP_LEARNING_RATE: num_lr,\n",
    "#                             HP_EPOCHS: num_ep,\n",
    "#                             HP_BATCH_SIZE: num_bs,\n",
    "#                         }\n",
    "#                         run_name = \"run-%d\" % session_num\n",
    "#                         print('--- Starting trial: %s' % run_name)\n",
    "#                         print({h.name: hparams[h] for h in hparams})\n",
    "#                         print('Arriva fin qui e perchè non prosegue? Come faccio a fargli piacere hparams? Ma soprattutto, è davvero colpa di hparams?')\n",
    "#                         run('logs/hparam_tuning/' + run_name, hparams)\n",
    "#                         session_num += 1\n",
    "\n",
    "# Ho provato a ridurre all'osso anche i parametri tunabili, limitandomi alla hidden size, ma non cambia nulla, il problema non sta lì, quindi. Dove allora?\n",
    "\n",
    "for num_hid in HP_HIDDEN_SIZE.domain.values:\n",
    "    hparams = {\n",
    "        HP_HIDDEN_SIZE: num_hid,\n",
    "    }\n",
    "    run_name = \"run-%d\" % session_num\n",
    "    print('--- Starting trial: %s' % run_name)\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    print('Arriva fin qui e perchè non prosegue? Come faccio a fargli piacere hparams? Ma soprattutto, è davvero colpa di hparams?')\n",
    "    run('logs/hparam_tuning/' + run_name, hparams)\n",
    "    session_num += 1                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "283988aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T09:51:55.964616Z",
     "start_time": "2022-06-24T09:51:55.953117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "#%tensorboard --logdir logsTf\n",
    "%load_ext tensorboard\n",
    "#Avvio da terminale: spostarsi nella cartella dove è presente la cartella di log, > tensorboard --logdir nomecartella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd27c49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T09:50:06.403200Z",
     "start_time": "2022-06-24T09:50:06.403200Z"
    }
   },
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9acba0606d7acdc9122521c1e55edc08153e62462fdeed28528cdde632651f64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
