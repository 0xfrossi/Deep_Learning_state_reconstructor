1:
#PARAM:
#tf.keras.regularizers.L1(l1=1e-4)
param={"INPUT_SIZE" : "4,340" ,
"OUTPUT_SIZE":340,
"HIDDEN_SIZE" : 670,
"HIDDEN_SIZE2" : 134,
"HIDDEN_SIZE3" : None,
"CODE_SIZE" : 67,
"KERNEL_INIZIALIZER":"he_uniform",
"KERNEL_REGULIZER":tf.keras.regularizers.L2(l2=1e-5),
'BIAS_REGULARIZER': None,
"ACTIVATION":'relu',
"OPTIMIZER":tf.keras.optimizers.Adam(learning_rate=0.001),
"EPOCHS":400,
"BATCH_SIZE":150,
"BATCH_NORMAIZATION": 0
}
Risultati: 
Numero di vettori decodificati correttamente: 21921 su 51231 totali 
Rapporto: 42.789% 
Media num di errori per array: 1.598

2:
#PARAM:
#tf.keras.regularizers.L1(l1=1e-4)
param={"INPUT_SIZE" : "4,340" ,
"OUTPUT_SIZE":340,
"HIDDEN_SIZE" : 670,
"HIDDEN_SIZE2" : 134,
"HIDDEN_SIZE3" : None,
"CODE_SIZE" : 67,
"KERNEL_INIZIALIZER":"he_uniform",
"KERNEL_REGULIZER":tf.keras.regularizers.L2(l2=1e-5),
'BIAS_REGULARIZER': None,
"ACTIVATION":'relu',
"OPTIMIZER":tf.keras.optimizers.Adam(learning_rate=0.1),
"EPOCHS":400,
"BATCH_SIZE":150,
"BATCH_NORMAIZATION": 0
}
Risultati:
Numero di vettori decodificati correttamente: 0 su 51231 totali 
Rapporto: 0.000% 
Media num di errori per array: 16.487
NOPE :)

3:
#PARAM:
#tf.keras.regularizers.L1(l1=1e-4)
param={"INPUT_SIZE" : "4,340" ,
"OUTPUT_SIZE":340,
"HIDDEN_SIZE" : 670,
"HIDDEN_SIZE2" : 134,
"HIDDEN_SIZE3" : None,
"CODE_SIZE" : 67,
"KERNEL_INIZIALIZER":"he_uniform",
"KERNEL_REGULIZER":tf.keras.regularizers.L2(l2=1e-5),
'BIAS_REGULARIZER': None,
"ACTIVATION":'relu',
"OPTIMIZER":tf.keras.optimizers.Adam(learning_rate=0.01),
"EPOCHS":400,
"BATCH_SIZE":150,
"BATCH_NORMAIZATION": 0
}
Risultati:
Numero di vettori decodificati correttamente: 1827 su 51231 totali 
Rapporto: 3.566% 
Media num di errori per array: 8.465
NOPE DI NUOVO :) MI SA CHE LEARNING RATE GRANDI GLI FANNO PROPRIO TANTO SCHIFO

4:
#PARAM:
#tf.keras.regularizers.L1(l1=1e-4)
param={"INPUT_SIZE" : "4,340" ,
"OUTPUT_SIZE":340,
"HIDDEN_SIZE" : 670,
"HIDDEN_SIZE2" : 134,
"HIDDEN_SIZE3" : None,
"CODE_SIZE" : 67,
"KERNEL_INIZIALIZER":"he_uniform",
"KERNEL_REGULIZER":tf.keras.regularizers.L2(l2=1e-5),
'BIAS_REGULARIZER': None,
"ACTIVATION":'relu',
"OPTIMIZER":tf.keras.optimizers.Adam(learning_rate=0.0001),
"EPOCHS":400,
"BATCH_SIZE":150,
"BATCH_NORMAIZATION": 0
}
Risultati:
Numero di vettori decodificati correttamente: 43446 su 51231 totali 
Rapporto: 84.804% 
Media num di errori per array: 0.479
ALMOST THERE. PROBABILMENTE È IL RANGE GIUSTO SU CUI MUOVERSI PER LA LEARNING RATE.
ANCORA UN TEST CON UNA LERNING RATE SIMILE, MAGARI UN PO' PIÙ GRANDE, PER VEDERE SE L'EARLY STOPPING KICKS IN E NON DEVE FARE TUTTE E 400 LE ITERAZIONI, SENZA PERDERE IN PERFORMANCE.

5:
#PARAM:
#tf.keras.regularizers.L1(l1=1e-4)
param={"INPUT_SIZE" : "4,340" ,
"OUTPUT_SIZE":340,
"HIDDEN_SIZE" : 670,
"HIDDEN_SIZE2" : 134,
"HIDDEN_SIZE3" : None,
"CODE_SIZE" : 67,
"KERNEL_INIZIALIZER":"he_uniform",
"KERNEL_REGULIZER":tf.keras.regularizers.L2(l2=1e-5),
'BIAS_REGULARIZER': None,
"ACTIVATION":'relu',
"OPTIMIZER":tf.keras.optimizers.Adam(learning_rate=0.0005),
"EPOCHS":400,
"BATCH_SIZE":150,
"BATCH_NORMAIZATION": 0
}
Risultati:
Numero di vettori decodificati correttamente: 39409 su 51231 totali 
Rapporto: 76.924% 
Media num di errori per array: 0.495
FORSE ALZATO TROPPO. ANCORA UNA PROVA INTERMEDIA PER VEDERE SE SI ASSESTA SULL'80%, PRIMA DI FARE I TENTATIVI COL REGOLARIZZATORE.

6:
#PARAM:
#tf.keras.regularizers.L1(l1=1e-4)
param={"INPUT_SIZE" : "4,340" ,
"OUTPUT_SIZE":340,
"HIDDEN_SIZE" : 670,
"HIDDEN_SIZE2" : 134,
"HIDDEN_SIZE3" : None,
"CODE_SIZE" : 67,
"KERNEL_INIZIALIZER":"he_uniform",
"KERNEL_REGULIZER":tf.keras.regularizers.L2(l2=1e-5),
'BIAS_REGULARIZER': None,
"ACTIVATION":'relu',
"OPTIMIZER":tf.keras.optimizers.Adam(learning_rate=0.0002),
"EPOCHS":400,
"BATCH_SIZE":150,
"BATCH_NORMAIZATION": 0
}
Risultati:
Numero di vettori decodificati correttamente: 41432 su 51231 totali 
Rapporto: 80.873% 
Media num di errori per array: 0.483
ECCOLO L'80%! a QUESTO PUNTO PROCEDO A TESTARE LA REGOLARIZZAZIONE.
