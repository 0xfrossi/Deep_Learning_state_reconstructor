{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f435793",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-22T07:32:59.387487Z",
     "start_time": "2022-06-22T07:32:56.343304Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils_functions import *\n",
    "from plan import *\n",
    "import random\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import datetime, os\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "349ba869",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-22T07:34:04.078007Z",
     "start_time": "2022-06-22T07:34:04.064508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# controllo di star effettivamente usando la GPU\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7177344b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-22T07:34:17.044405Z",
     "start_time": "2022-06-22T07:34:11.722905Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded\n",
      "File loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dict_stati= load_file(\"./dizionario_stati\")\n",
    "plansLoaded=load_file(\"./plans\")\n",
    "#print(type(plansPickles))\n",
    "#plansList=load_from_pickles(\"C:/Users/Francesco/Desktop/dataset/dataset/dizionario_stati\",plansPickles)\n",
    "#print(plansPickles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "994cd7db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-22T07:34:17.059908Z",
     "start_time": "2022-06-22T07:34:17.046407Z"
    }
   },
   "outputs": [],
   "source": [
    "#ogni stato è un tensore con elementi dtype int8, questi stati vengono raggruppati in un altro tensore che rappresnta la\n",
    "#variabile \"states\" del piano \n",
    "def build_vector(dict, states_list):\n",
    "    l=len(dict)\n",
    "    vector_states=[]\n",
    "    for state in states_list:\n",
    "        vector=np.array([0]*l,dtype=np.int8)\n",
    "        #vector=tf.zeros(l,dtype=np.int8)\n",
    "        for s in state:\n",
    "            for key in dict.keys():\n",
    "                if key==s:\n",
    "                    vector[dict[key]-1]=1\n",
    "                    break\n",
    "        #t=tf.convert_to_tensor(vector,dtype=tf.int8)            \n",
    "        vector_states.append(vector) \n",
    "    r = np.array(vector_states)                          \n",
    "    return r\n",
    "\n",
    "\n",
    "#NON UTILIZZATO ADESSO\n",
    "#shape di ogni singolo elemento (r) è (n x 340) con n che varia su ogni piano, raggruppati sulla base dei piani\n",
    "def build_all_vectors(dict,plans_list):\n",
    "    total=[]\n",
    "    for plan in plans_list:\n",
    "        r=build_vector(dict,plan.states)\n",
    "        total.append(r)\n",
    "    #r = tf.convert_to_tensor(total,dtype=None)                         \n",
    "    return total\n",
    " \n",
    "    \n",
    "#gli stati vengono ordinati (in una lista) singolarmente con shape (1x340), non vengono raggruppati sulla base dei piani\n",
    "#da utilizzare per autoencoder standard\n",
    "def build_all_vectors1x340(dict, plans_list):\n",
    "    l=len(dict)\n",
    "    total=[]\n",
    "    for plan in plans_list:\n",
    "        for state in plan.states:\n",
    "            vector=np.array([0]*l,dtype=np.int8)\n",
    "            for s in state:\n",
    "                for key in dict.keys():\n",
    "                    if key==s:\n",
    "                        vector[dict[key]-1]=1\n",
    "                        break\n",
    "            #t=tf.convert_to_tensor(vector,dtype=tf.int8)            \n",
    "            total.append(vector)\n",
    "    total=np.array(total)        \n",
    "    return total        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8f80382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-22T07:34:17.137378Z",
     "start_time": "2022-06-22T07:34:17.122378Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all_states_from_plans_list=build_all_vectors(dict_stati,plansLoaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe83a51",
   "metadata": {},
   "source": [
    "## Preparazione dati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c8c33",
   "metadata": {},
   "source": [
    "### Caricamento dataset statici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a1464b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-22T07:36:38.063103Z",
     "start_time": "2022-06-22T07:34:24.953920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded\n",
      "File loaded\n",
      "File loaded\n"
     ]
    }
   ],
   "source": [
    "test=load_file(\"./Dataset/set_test\")\n",
    "train=load_file(\"./Dataset/set_training\")\n",
    "validation=load_file(\"./Dataset/set_validation\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eda145f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-22T07:38:07.897786Z",
     "start_time": "2022-06-22T07:37:21.958785Z"
    }
   },
   "outputs": [],
   "source": [
    "test=np.array(test,dtype=np.int8)\n",
    "train=np.array(train,dtype=np.int8)\n",
    "validation=np.array(validation,dtype=np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85be87a",
   "metadata": {},
   "source": [
    "## Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00dc9c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-22T07:37:21.324142Z",
     "start_time": "2022-06-22T07:36:38.065104Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 41>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#variare learning_rate beta_1,beta_2, da fare per ultimo\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#batch_size ???\u001b[39;00m\n\u001b[0;32m     40\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 41\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\EnvAutoEncoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1143\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_coordinator \u001b[38;5;241m=\u001b[39m cluster_coordinator\u001b[38;5;241m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1138\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy)\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), \\\n\u001b[0;32m   1141\u001b[0m      training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1142\u001b[0m   \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1143\u001b[0m   data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m      \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m      \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m      \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1159\u001b[0m   \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\EnvAutoEncoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1400\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1399\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\EnvAutoEncoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1155\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1152\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_value \u001b[38;5;241m=\u001b[39m steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m   1154\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1169\u001b[0m strategy \u001b[38;5;241m=\u001b[39m ds_context\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\EnvAutoEncoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:339\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m    337\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[1;32m--> 339\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    342\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshuffle_batch\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\EnvAutoEncoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:365\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mslice_inputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices_dataset, inputs):\n\u001b[0;32m    349\u001b[0m   \u001b[38;5;124;03m\"\"\"Slice inputs into a Dataset of batches.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m  Given a Dataset of batch indices and the unsliced inputs,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    A Dataset of input batches matching the batch indices.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m    363\u001b[0m   dataset \u001b[38;5;241m=\u001b[39m dataset_ops\u001b[38;5;241m.\u001b[39mDatasetV2\u001b[38;5;241m.\u001b[39mzip((\n\u001b[0;32m    364\u001b[0m       indices_dataset,\n\u001b[1;32m--> 365\u001b[0m       \u001b[43mdataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDatasetV2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrepeat()\n\u001b[0;32m    366\u001b[0m   ))\n\u001b[0;32m    368\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m d: array_ops\u001b[38;5;241m.\u001b[39mgather(d, i, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), data)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\EnvAutoEncoder\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:606\u001b[0m, in \u001b[0;36mDatasetV2.from_tensors\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_tensors\u001b[39m(tensors):\n\u001b[0;32m    572\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a `Dataset` with a single element, comprising the given tensors.\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \n\u001b[0;32m    574\u001b[0m \u001b[38;5;124;03m  `from_tensors` produces a dataset containing only a single element. To slice\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;124;03m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 606\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\EnvAutoEncoder\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3825\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[1;34m(self, element)\u001b[0m\n\u001b[0;32m   3823\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element):\n\u001b[0;32m   3824\u001b[0m   \u001b[38;5;124;03m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3825\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3826\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[0;32m   3827\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure, element)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\EnvAutoEncoder\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:130\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    127\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    128\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    129\u001b[0m             ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponent_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m i, dtype\u001b[38;5;241m=\u001b[39mdtype))\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack_sequence_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpack_as\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_components\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\EnvAutoEncoder\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py:176\u001b[0m, in \u001b[0;36mpack_sequence_as\u001b[1;34m(structure, flat_sequence)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(flat_structure) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(flat_sequence):\n\u001b[0;32m    171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    172\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not pack sequence. Structure had \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m elements, but flat_sequence \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    173\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhad \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m elements.  Structure: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, flat_sequence: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m       \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(flat_structure), \u001b[38;5;28mlen\u001b[39m(flat_sequence), structure, flat_sequence))\n\u001b[1;32m--> 176\u001b[0m _, packed \u001b[38;5;241m=\u001b[39m \u001b[43m_packed_nest_with_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39m_sequence_like(structure, packed)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\EnvAutoEncoder\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py:133\u001b[0m, in \u001b[0;36m_packed_nest_with_indices\u001b[1;34m(structure, flat, index)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m _yield_value(structure):\n\u001b[0;32m    132\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m is_sequence(s):\n\u001b[1;32m--> 133\u001b[0m     new_index, child \u001b[38;5;241m=\u001b[39m \u001b[43m_packed_nest_with_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m     packed\u001b[38;5;241m.\u001b[39mappend(nest\u001b[38;5;241m.\u001b[39m_sequence_like(s, child))  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     index \u001b[38;5;241m=\u001b[39m new_index\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\EnvAutoEncoder\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py:137\u001b[0m, in \u001b[0;36m_packed_nest_with_indices\u001b[1;34m(structure, flat, index)\u001b[0m\n\u001b[0;32m    135\u001b[0m     index \u001b[38;5;241m=\u001b[39m new_index\n\u001b[0;32m    136\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[43mpacked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(flat[index])\n\u001b[0;32m    138\u001b[0m     index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, packed\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: Cercare come migliorare l'autoencoder e come interpretare le metriche, \n",
    "# i test per il momento sono abbastanza a meno che non vuoi provare qualcosa di specifico sulla base di qello che trovi\n",
    "\n",
    "#****************************************************************************************************\n",
    "\n",
    "logdir = os.path.join(\"./TestLogs/\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=10),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=logdir,histogram_freq=1, write_graph=True, write_images=True)\n",
    "\n",
    "]\n",
    "\n",
    "#Cose da fare per migliorare la rete solo dopo aver fatto i primi tentativi con la rete proposta:\n",
    "# * Provare ad aggiungere Regolarizzazione es. L1,L2 e dropout(solo nella fase di encoding)\n",
    "# * Provare swish al posto di relu\n",
    "# * Provare tf.keras.layers.BatchNormalization()(var_layer) \n",
    "# * Fare Hyperparameter Tuning  (ultima) www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams\n",
    "#Salvare i risultati con ogni modifica fatta per scriverli nel report\n",
    "\n",
    "input_size = 340 #costante\n",
    "\n",
    "#Scelti casulmente al momento.... da rivedere come prima modifica forse\n",
    "hidden_size = 170\n",
    "#hidden_size2 = 85\n",
    "code_size = 85\n",
    "\n",
    "input_layer = Input(shape=(input_size,))\n",
    "hidden_1 = Dense(hidden_size, activation='swish', kernel_initializer=\"he_uniform\", )(input_layer)\n",
    "#hidden_3 = Dense(hidden_size, activation='relu', kernel_initializer=\"he_uniform\")(hidden_1)\n",
    "\n",
    "code = Dense(code_size, activation='swish',kernel_initializer=\"he_uniform\",)(hidden_1)\n",
    "hidden_2 =Dense(hidden_size, activation='swish',kernel_initializer=\"he_uniform\",)(code)\n",
    "#hidden_4 =Dense(hidden_size, activation='relu',kernel_initializer=\"he_uniform\")(hidden_2)\n",
    "\n",
    "output_layer = Dense(input_size, activation='sigmoid')(hidden_2)\n",
    "\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "#variare learning_rate beta_1,beta_2, da fare per ultimo\n",
    "#batch_size ???\n",
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy',metrics=[\"accuracy\",\"Precision\",\"Recall\"])\n",
    "autoencoder.fit(x=train,y=train, epochs=500,batch_size=300000, validation_data=(validation,validation), callbacks=my_callbacks,\n",
    "                      )\n",
    "\n",
    " #Gardare i risulati delle metriche su tensorbord                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283988aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-18T13:15:32.761190Z",
     "start_time": "2022-06-18T13:15:32.749190Z"
    }
   },
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logsTf\n",
    "%load_ext tensorboard\n",
    "#Avvio da terminale: spostarsi nella cartella dove è presente la cartella di log, > tensorboard --logdir nomecartella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd27c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9acba0606d7acdc9122521c1e55edc08153e62462fdeed28528cdde632651f64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
